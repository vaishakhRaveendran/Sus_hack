{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMeapHz4u8kLLVveT76Vk6f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaishakhRaveendran/Sus_hack/blob/main/cotton_india.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna --quiet\n",
        "!pip install umap-learn --quiet"
      ],
      "metadata": {
        "id": "Wn83R67dBCDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor,RandomForestClassifier\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.impute import KNNImputer,SimpleImputer\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "sklearn.set_config(transform_output=\"pandas\")\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import optuna\n",
        "from functools import partial\n",
        "import gc"
      ],
      "metadata": {
        "id": "P3lgRKfhBr4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FILEPATH='https://raw.githubusercontent.com/vaishakhRaveendran/Sus_hack/main'"
      ],
      "metadata": {
        "id": "nDn72pcdJE2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0SoaQc6zfyq"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(os.path.join(FILEPATH,'India_train.csv'))\n",
        "df_test = pd.read_csv(os.path.join(FILEPATH, 'India_test.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.columns.tolist()"
      ],
      "metadata": {
        "id": "2k11P6DSUGv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import FunctionTransformer\n",
        "#Advanced Pipelines\n",
        "from sklearn import set_config\n",
        "set_config(display=\"diagram\")\n",
        "from sklearn.pipeline import make_pipeline,Pipeline\n",
        "from sklearn.compose import ColumnTransformer,make_column_selector\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder,MinMaxScaler\n",
        "\n",
        "class adv_pipe():\n",
        "  def __init__(self):\n",
        "    return None\n",
        "\n",
        "  def create_pipe(self):\n",
        "    self.adv_ppl=Pipeline(steps=[\n",
        "     ('Column_transform',ColumnTransformer(transformers=[\n",
        "           ('num',make_pipeline(SimpleImputer(strategy='constant'),MinMaxScaler()),make_column_selector(dtype_include='float')),\n",
        "           ('cat',make_pipeline(SimpleImputer(strategy='most_frequent'),OrdinalEncoder()),make_column_selector(dtype_include='object'))],\n",
        "    remainder='passthrough',\n",
        "    verbose_feature_names_out=False)),\n",
        "    ('Reduce_col',reduce_col(0.007)),\n",
        "    ('dimension_reduction',Decomp())\n",
        "    ])\n",
        "\n",
        "    return self.adv_ppl\n",
        "\n"
      ],
      "metadata": {
        "id": "xw01f3Bh2xn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Groups():\n",
        "  def __init__(self,df_train,df_test,category,random_state=42,test_size=0.33):\n",
        "    self.category=category\n",
        "    self.gp_train=df_train.groupby(self.category)\n",
        "    self.gp_test=df_test.groupby(self.category)\n",
        "    self.random_state=random_state\n",
        "    self.test_size=test_size\n",
        "    self.target_list=df_train[self.category].unique().tolist()\n",
        "\n",
        "  def Split(self,impute=False):\n",
        "    if impute:\n",
        "       for name in self.target_list:\n",
        "        A=self.gp_train.get_group(name)\n",
        "        B=self.gp_test.get_group(name)\n",
        "        yield A,B\n",
        "\n",
        "    else:\n",
        "      for state in self.target_list:\n",
        "        ppl=adv_pipe().create_pipe()\n",
        "        X=self.gp_train.get_group(state).drop(['COTTON YIELD (Kg per ha)'],inplace=False,axis=1)\n",
        "        y=self.gp_train.get_group(state)['COTTON YIELD (Kg per ha)']\n",
        "        test= self.gp_test.get_group(state)\n",
        "        X=ppl.fit_transform(X)\n",
        "        test=ppl.transform(test)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33, random_state=42)\n",
        "        yield state,X_train, X_test, y_train, y_test,test\n"
      ],
      "metadata": {
        "id": "DqKM8Kkr2yMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normal transformers cant be used for stateful transformations\n",
        "from sklearn.base import TransformerMixin,BaseEstimator\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "class reduce_col(TransformerMixin):\n",
        "  def __init__(self,threshold=0.0):\n",
        "    self.threshold=threshold\n",
        "\n",
        "  def fit(self,X,y=None):\n",
        "     col_vars=X.var()\n",
        "     self.col_to_drop = col_vars[col_vars.values<self.threshold].index\n",
        "     return self\n",
        "\n",
        "  def transform(self,X):\n",
        "    assert self.col_to_drop is not None, 'Drop_col error, must be fitted before predict'\n",
        "    X.drop(self.col_to_drop, axis=1, inplace=True)\n",
        "    return X\n",
        "\n"
      ],
      "metadata": {
        "id": "3cQ_IaAJInNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap.umap_ as umap\n",
        "class Decomp(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_components=20, method=\"umap\", col_drop=['Year','COTTON PRODUCTION (1000 tons)']):\n",
        "        self.n_components = n_components\n",
        "        self.method = method\n",
        "        self.col_drop = col_drop\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X_temp = X.drop(self.col_drop, axis=1) if self.col_drop else X.copy()\n",
        "        if self.method == \"pca\":\n",
        "            self.comp = PCA(n_components=self.n_components, random_state=0)\n",
        "        elif self.method == \"umap\":\n",
        "            self.comp = umap.UMAP(n_components=self.n_components, random_state=0)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid method name: {self.method}\")\n",
        "        self.comp.fit(X_temp)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_temp = pd.concat([X.pop(x) for x in self.col_drop], axis=1)\n",
        "        X_array = np.asarray(X)\n",
        "        X_reduced = self.comp.transform(X_array)\n",
        "        columns = [f\"{self.method.upper()}_{i}\" for i in range(self.n_components)]\n",
        "        X_reduced = pd.DataFrame(X_reduced, columns=columns, index=X_temp.index)\n",
        "        return pd.concat([X_temp,X_reduced],axis=1)\n"
      ],
      "metadata": {
        "id": "xepaMpYSKSdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Regressor:\n",
        "    def __init__(self):\n",
        "        self.models = self._define_model()\n",
        "        self.models_name = list(self._define_model().keys())\n",
        "        self.len_models = len(self.models)\n",
        "\n",
        "    def _define_model(self):\n",
        "\n",
        "        xgb_params = {\n",
        "        }\n",
        "        lgbm_params={\n",
        "\n",
        "        }\n",
        "\n",
        "\n",
        "        rf_params =  {\n",
        "\n",
        "\n",
        "        }\n",
        "\n",
        "        gdbt_params = {\n",
        "\n",
        "        }\n",
        "\n",
        "\n",
        "        models = {\n",
        "            'xgb':  XGBRegressor(**xgb_params),\n",
        "            'gdbt': GradientBoostingRegressor(**gdbt_params),\n",
        "            'rf' : RandomForestRegressor(**rf_params),\n",
        "            'lgbm': LGBMRegressor(**lgbm_params)\n",
        "        }\n",
        "\n",
        "        return models"
      ],
      "metadata": {
        "id": "evoxjHeCM3cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OptunaWeights:\n",
        "    def __init__(self, random_state, n_trials=2000):\n",
        "        self.study = None\n",
        "        self.weights = None\n",
        "        self.random_state = random_state\n",
        "        self.n_trials = n_trials\n",
        "\n",
        "    def _objective(self, trial, y_true, y_preds):\n",
        "        # Define the weights for the predictions from each model\n",
        "        weights = [trial.suggest_float(f\"weight{n}\", 1e-14, 1) for n in range(len(y_preds))]\n",
        "\n",
        "        # Calculate the weighted prediction\n",
        "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n",
        "\n",
        "        # Calculate the score for the weighted prediction\n",
        "        score = mean_absolute_error(y_true, weighted_pred)\n",
        "        return score\n",
        "\n",
        "    def fit(self, y_true, y_preds):\n",
        "        optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
        "        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n",
        "        pruner = optuna.pruners.HyperbandPruner()\n",
        "        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\", direction='minimize')\n",
        "        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n",
        "        self.study.optimize(objective_partial, n_trials=self.n_trials)\n",
        "        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n",
        "\n",
        "    def predict(self, y_preds):\n",
        "        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n",
        "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n",
        "        return weighted_pred\n",
        "\n",
        "    def fit_predict(self, y_true, y_preds):\n",
        "        self.fit(y_true, y_preds)\n",
        "        return self.predict(y_preds)\n",
        "\n",
        "    def weights(self):\n",
        "        return self.weights"
      ],
      "metadata": {
        "id": "YxLzz_cJHcbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Gp=Groups(df_train,df_test,'State Name',10,0.4)\n",
        "test_predss={}\n",
        "ensemble_score={}\n",
        "for i, (state,X_train, X_val, y_train, y_val,test) in enumerate(Gp.Split()):\n",
        "    regressor = Regressor()\n",
        "    models = regressor.models\n",
        "    test_preds = []\n",
        "    oof_preds = []\n",
        "    print(state,X_train.columns)\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_val_pred =model.predict(X_val).reshape(-1)\n",
        "        test_pred = model.predict(test).reshape(-1)\n",
        "        oof_preds.append(y_val_pred)\n",
        "        test_preds.append(test_pred)\n",
        "\n",
        "    optweights = OptunaWeights(random_state=42)\n",
        "    y_val_pred = optweights.fit_predict(y_val.values, oof_preds)\n",
        "    score = mean_squared_error(y_val, y_val_pred)\n",
        "    ensemble_score[state]=score\n",
        "    test_predss[state]=optweights.predict(test_preds)\n",
        "    gc.collect()\n"
      ],
      "metadata": {
        "id": "MhtD7_r5J5kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_score"
      ],
      "metadata": {
        "id": "USar1BtYTYyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(list(ensemble_score.values())).mean()"
      ],
      "metadata": {
        "id": "31ek-0u5HwDN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}